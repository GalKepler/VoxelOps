{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QSIParc Basics: Parcellation and Regional Analysis\n",
    "\n",
    "This notebook demonstrates how to use the QSIParc runner for diffusion-weighted parcellation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "QSIParc performs:\n",
    "- Parcellation using the `parcellate` tool\n",
    "- Regional quantification of diffusion metrics\n",
    "- Atlas registration and labeling\n",
    "- Generation of region-wise statistics\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Docker installed and running\n",
    "- QSIRecon reconstructed data\n",
    "- FreeSurfer license file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from voxelops import (\n",
    "    run_qsiparc,\n",
    "    QSIParcInputs,\n",
    "    QSIParcDefaults,\n",
    ")\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input paths\n",
    "qsirecon_dir = Path(\"/media/storage/yalab-dev/qsiprep_test/qsirecon_output/\")\n",
    "participant = \"01\"\n",
    "fs_license = Path(\"/home/galkepler/misc/freesurfer/license.txt\")\n",
    "\n",
    "# Output paths (optional)\n",
    "output_dir = Path(\"/media/storage/yalab-dev/qsiprep_tes/qsiparc\")\n",
    "work_dir = Path(\"/media/storage/yalab-dev/qsiprep_test/work/qsiparc/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "### Option 1: Use Default Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Running qsiparc for participant 01\n",
      "================================================================================\n",
      "Command: docker run --rm -v /media/storage/yalab-dev/qsiprep_test/qsirecon_output:/input:ro -v /media/storage/yalab-dev/qsiprep_tes/qsiparc:/output pennlinc/qsiparc:latest /input /output --participant-label=01 --atlas schaefer100 --atlas schaefer200\n",
      "================================================================================\n",
      "\n",
      "Execution log saved: /media/storage/yalab-dev/qsiprep_tes/logs/qsiparc_01_20260201_134616.json\n"
     ]
    },
    {
     "ename": "ProcedureExecutionError",
     "evalue": "qsiparc failed: qsiparc failed with exit code 125\n\nStderr (last 1000 chars):\nUnable to find image 'pennlinc/qsiparc:latest' locally\ndocker: Error response from daemon: pull access denied for pennlinc/qsiparc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied.\nSee 'docker run --help'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mProcedureExecutionError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m inputs = QSIParcInputs(\n\u001b[32m      3\u001b[39m     qsirecon_dir=qsirecon_dir,\n\u001b[32m      4\u001b[39m     participant=participant,\n\u001b[32m      5\u001b[39m     output_dir=output_dir,\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# work_dir=work_dir,\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Run with defaults\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m result = \u001b[43mrun_qsiparc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfs_license\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfs_license\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccess: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDuration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mduration_human\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/yalab-devops/VoxelOps/src/voxelops/runners/qsiparc.py:94\u001b[39m, in \u001b[36mrun_qsiparc\u001b[39m\u001b[34m(inputs, config, **overrides)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Execute\u001b[39;00m\n\u001b[32m     93\u001b[39m log_dir = output_dir.parent / \u001b[33m\"\u001b[39m\u001b[33mlogs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m result = \u001b[43mrun_docker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mqsiparc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparticipant\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparticipant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Add inputs, config, and expected outputs to result\u001b[39;00m\n\u001b[32m    102\u001b[39m result[\u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m] = inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/yalab-devops/VoxelOps/src/voxelops/runners/_base.py:148\u001b[39m, in \u001b[36mrun_docker\u001b[39m\u001b[34m(cmd, tool_name, participant, log_dir, capture_output)\u001b[39m\n\u001b[32m    145\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(log_file, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    146\u001b[39m             json.dump(record, f, indent=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ProcedureExecutionError(\n\u001b[32m    149\u001b[39m         procedure_name=tool_name,\n\u001b[32m    150\u001b[39m         message=error_msg,\n\u001b[32m    151\u001b[39m     )\n\u001b[32m    153\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    154\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m completed successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mProcedureExecutionError\u001b[39m: qsiparc failed: qsiparc failed with exit code 125\n\nStderr (last 1000 chars):\nUnable to find image 'pennlinc/qsiparc:latest' locally\ndocker: Error response from daemon: pull access denied for pennlinc/qsiparc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied.\nSee 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "# Create inputs\n",
    "inputs = QSIParcInputs(\n",
    "    qsirecon_dir=qsirecon_dir,\n",
    "    participant=participant,\n",
    "    output_dir=output_dir,\n",
    "    # work_dir=work_dir,\n",
    ")\n",
    "\n",
    "# Run with defaults\n",
    "result = run_qsiparc(\n",
    "    inputs,\n",
    "    fs_license=fs_license,\n",
    ")\n",
    "\n",
    "print(f\"Success: {result['success']}\")\n",
    "print(f\"Duration: {result['duration_human']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Override Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with specific resources\n",
    "result = run_qsiparc(\n",
    "    inputs,\n",
    "    fs_license=fs_license,\n",
    "    nprocs=16,\n",
    "    mem_gb=32,\n",
    ")\n",
    "\n",
    "print(f\"Success: {result['success']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Custom Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom configuration\n",
    "config = QSIParcDefaults(\n",
    "    nprocs=12,\n",
    "    mem_gb=24,\n",
    "    skip_bids_validation=False,\n",
    "    fs_license=fs_license,\n",
    "    docker_image=\"pennlinc/qsiparc:latest\",\n",
    ")\n",
    "\n",
    "result = run_qsiparc(inputs, config)\n",
    "\n",
    "print(f\"Success: {result['success']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Execution Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Execution Details:\")\n",
    "print(f\"  Tool: {result['tool']}\")\n",
    "print(f\"  Participant: {result['participant']}\")\n",
    "print(f\"  Duration: {result['duration_human']}\")\n",
    "print(f\"  Success: {result['success']}\")\n",
    "\n",
    "print(\"\\nConfiguration Used:\")\n",
    "config_used = result[\"config\"]\n",
    "print(f\"  Cores: {config_used.nprocs}\")\n",
    "print(f\"  Memory: {config_used.mem_gb}GB\")\n",
    "print(f\"  Docker image: {config_used.docker_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Expected Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = result[\"expected_outputs\"]\n",
    "\n",
    "print(\"Expected Output Locations:\")\n",
    "print(f\"  QSIParc directory: {outputs.qsiparc_dir}\")\n",
    "print(f\"  Participant directory: {outputs.participant_dir}\")\n",
    "print(f\"  Work directory: {outputs.work_dir}\")\n",
    "\n",
    "# Verify outputs\n",
    "print(\"\\nOutput Validation:\")\n",
    "print(f\"  Participant dir exists: {outputs.participant_dir.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Parcellation Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outputs.participant_dir.exists():\n",
    "    print(f\"Parcellation outputs for {participant}:\\n\")\n",
    "\n",
    "    # List all files\n",
    "    parcellation_maps = []\n",
    "    statistics = []\n",
    "    other = []\n",
    "\n",
    "    for f in outputs.participant_dir.rglob(\"*\"):\n",
    "        if f.is_file():\n",
    "            if f.suffix == \".nii.gz\":\n",
    "                parcellation_maps.append(f)\n",
    "            elif f.suffix in [\".csv\", \".tsv\"]:\n",
    "                statistics.append(f)\n",
    "            else:\n",
    "                other.append(f)\n",
    "\n",
    "    print(f\"Parcellation Maps ({len(parcellation_maps)}):\")\n",
    "    for f in sorted(parcellation_maps):\n",
    "        print(f\"  {f.name}\")\n",
    "\n",
    "    print(f\"\\nStatistics Files ({len(statistics)}):\")\n",
    "    for f in sorted(statistics):\n",
    "        print(f\"  {f.name}\")\n",
    "\n",
    "    if other:\n",
    "        print(f\"\\nOther Files ({len(other)}):\")\n",
    "        for f in sorted(other)[:5]:  # Show first 5\n",
    "            print(f\"  {f.name}\")\n",
    "else:\n",
    "    print(\"Participant directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Analyze Regional Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find statistics files\n",
    "stats_files = list(outputs.participant_dir.rglob(\"*.csv\"))\n",
    "\n",
    "if stats_files:\n",
    "    # Load first statistics file\n",
    "    stats_file = stats_files[0]\n",
    "    print(f\"Loading: {stats_file.name}\\n\")\n",
    "\n",
    "    # Load data\n",
    "    stats_df = pd.read_csv(stats_file)\n",
    "\n",
    "    print(f\"Shape: {stats_df.shape}\")\n",
    "    print(f\"Columns: {list(stats_df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(stats_df.head())\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    display(stats_df.describe())\n",
    "else:\n",
    "    print(\"No statistics files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Regional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if stats_files and \"stats_df\" in locals():\n",
    "    # Assume there's a 'region' and some metric columns\n",
    "    # Adjust based on actual column names\n",
    "\n",
    "    # Example: Plot distribution of FA values across regions\n",
    "    if \"FA\" in stats_df.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Histogram\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(stats_df[\"FA\"].dropna(), bins=30, edgecolor=\"black\")\n",
    "        plt.xlabel(\"Fractional Anisotropy (FA)\")\n",
    "        plt.ylabel(\"Number of Regions\")\n",
    "        plt.title(\"Distribution of FA Across Regions\")\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Box plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.boxplot(stats_df[\"FA\"].dropna())\n",
    "        plt.ylabel(\"Fractional Anisotropy (FA)\")\n",
    "        plt.title(\"FA Distribution Summary\")\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nFA Statistics:\")\n",
    "        print(f\"  Mean: {stats_df['FA'].mean():.3f}\")\n",
    "        print(f\"  Std: {stats_df['FA'].std():.3f}\")\n",
    "        print(f\"  Min: {stats_df['FA'].min():.3f}\")\n",
    "        print(f\"  Max: {stats_df['FA'].max():.3f}\")\n",
    "\n",
    "    # If there are multiple metrics, create correlation matrix\n",
    "    numeric_cols = stats_df.select_dtypes(include=\"number\").columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        correlation = stats_df[numeric_cols].corr()\n",
    "        sns.heatmap(correlation, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
    "        plt.title(\"Correlation Matrix of Diffusion Metrics\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of participants from QSIRecon directory\n",
    "participant_dirs = sorted(qsirecon_dir.glob(\"sub-*\"))\n",
    "participants = [d.name.replace(\"sub-\", \"\") for d in participant_dirs if d.is_dir()]\n",
    "\n",
    "print(f\"Found {len(participants)} participants: {participants}\\n\")\n",
    "\n",
    "config = QSIParcDefaults(\n",
    "    nprocs=12,\n",
    "    mem_gb=24,\n",
    "    fs_license=fs_license,\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "for participant in participants:\n",
    "    print(f\"Processing participant {participant}...\")\n",
    "\n",
    "    inputs = QSIParcInputs(\n",
    "        qsirecon_dir=qsirecon_dir,\n",
    "        participant=participant,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = run_qsiparc(inputs, config)\n",
    "        results.append(result)\n",
    "        print(f\"  ✓ Success in {result['duration_human']}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed: {e}\\n\")\n",
    "        results.append(\n",
    "            {\n",
    "                \"participant\": participant,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Summary\n",
    "successful = sum(1 for r in results if r.get(\"success\"))\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Processed {len(results)} participants:\")\n",
    "print(f\"  ✓ Successful: {successful}\")\n",
    "print(f\"  ✗ Failed: {len(results) - successful}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Statistics Across Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect statistics from all successful runs\n",
    "all_stats = []\n",
    "\n",
    "for result in results:\n",
    "    if result.get(\"success\"):\n",
    "        participant = result[\"participant\"]\n",
    "        outputs = result[\"expected_outputs\"]\n",
    "\n",
    "        # Find statistics files\n",
    "        stats_files = list(outputs.participant_dir.rglob(\"*.csv\"))\n",
    "\n",
    "        for stats_file in stats_files:\n",
    "            df = pd.read_csv(stats_file)\n",
    "            df[\"participant\"] = participant\n",
    "            all_stats.append(df)\n",
    "\n",
    "if all_stats:\n",
    "    # Combine all dataframes\n",
    "    combined_stats = pd.concat(all_stats, ignore_index=True)\n",
    "\n",
    "    print(f\"Combined statistics: {combined_stats.shape}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(combined_stats.head())\n",
    "\n",
    "    # Save to file\n",
    "    output_file = output_dir / \"combined_regional_statistics.csv\"\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    combined_stats.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSaved combined statistics to: {output_file}\")\n",
    "else:\n",
    "    print(\"No statistics to combine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from voxelops.exceptions import (\n",
    "    ProcedureExecutionError,\n",
    "    InputValidationError,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = run_qsiparc(\n",
    "        inputs,\n",
    "        fs_license=fs_license,\n",
    "    )\n",
    "    print(f\"Success: {result['success']}\")\n",
    "\n",
    "except InputValidationError as e:\n",
    "    print(f\"Input validation failed: {e}\")\n",
    "    print(\"Common issues:\")\n",
    "    print(\"  - QSIRecon directory doesn't exist\")\n",
    "    print(\"  - Participant not found in QSIRecon output\")\n",
    "\n",
    "except ProcedureExecutionError as e:\n",
    "    print(f\"Execution failed: {e}\")\n",
    "    print(f\"Check logs: {result.get('log_file')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After parcellation:\n",
    "\n",
    "1. Analyze regional statistics across subjects\n",
    "2. Compare metrics between groups (e.g., patients vs controls)\n",
    "3. Correlate with behavioral or clinical measures\n",
    "4. Visualize parcellation overlays on structural images\n",
    "5. Export to statistical analysis software (R, SPSS, etc.)\n",
    "\n",
    "## Tips\n",
    "\n",
    "- **Atlas choice**: Different atlases provide different regional granularity\n",
    "- **Quality control**: Check parcellation overlays visually\n",
    "- **Statistics**: Export to CSV for easy analysis in R, Python, or SPSS\n",
    "- **Batch processing**: Process all subjects with the same configuration for consistency\n",
    "- **Version control**: Pin Docker image versions for reproducibility\n",
    "- **Data organization**: Maintain a consistent directory structure across subjects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VoxelOps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
